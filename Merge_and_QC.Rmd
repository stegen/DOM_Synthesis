---
title: "Data merging and QC script"
author: c("Robert Danczak", "Oliver J. Lechtenfeld", "James Stegen", "Andrew Tanenzap", "et al.")
date: "`r Sys.Date()`"
output: html_document
---

# Project Setup {-}
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
library(data.table)
library(here)
library(tidyverse)
library(ggpubr)
library(ggridges)
library(googledrive)
library(todor)
library(scales)

```

## Authentication
```{r authenticate google}

drive_auth() # use the same Google Account used to access the Google Drive Folder
# Note the you need to autheticate once (in Web browser with your google credentials). The second time you run this chunk, you are required to make a selection ("2") in the R console. 
# However, when knitting the markdown it uses the pre-cached token (with a note)

```


## Global Settings
```{r Global settings}

# Initiate molecular number threshold
Molecular_number_threshold <- 0

# Should original data be kept after trimming?
keep_orig_data <- F

# make sure files are up to date and downloaded to working directory during knitting
if (isTRUE(getOption('knitr.in.progress'))) { 

  download_files <- T
  upload_files <- F

} else {
  
# decide if files shall be updated (both directions) or not  
  download_files <- F # This is at least required upon first execution of the markdown
  upload_files <- F
  
}

```


# Download Input Files from Google Drive

Please update the links, if a different file shall be used!
Deactivate downloads, in case the files are re-generated on the fly in this script
```{r download csv files into working directory}

if (download_files == T) {

  # original data file not used in favor of cleaned file from ECF
  # Download from James Google Drive: https://drive.google.com/drive/folders/1xTahrifFXMWOzOj0GcLyg1WPA5-TEqF4?usp=drive_link
  # drive_download(as_id("https://drive.google.com/file/d/1BglQCdkUHMyP0vCLWvVzUSJ2K5m-LCaV"), overwrite = T) # Merged_Processed_Data - 10-19-23.csv
  
  # original data file with modified name modifications and averaging (ECF)
  # Download from James Google Drive: https://drive.google.com/drive/folders/1xTahrifFXMWOzOj0GcLyg1WPA5-TEqF4?usp=drive_link
  drive_download(as_id("https://drive.google.com/file/d/1Q0tUBe-pk76--9RGuUICqvBypodRSbNS"), overwrite = T) # Merged_Processed_Data - 10-19-23_ECF_ManyFilesFinal_Index.csv
  
  # original mol file
  # Download from James Google Drive: https://drive.google.com/drive/folders/1xTahrifFXMWOzOj0GcLyg1WPA5-TEqF4?usp=drive_link
  drive_download(as_id("https://drive.google.com/file/d/1MtbAViGO9Cw8B9p0mx6gcjda-DgXJYr9"), overwrite = T) # Merged_Processed_Mol - 10-19-23.csv
  
  # trimmed data not used as trimming is implemented below
  # drive_download(as_id("https://drive.google.com/file/d/1AaJYEK0IM5TiZL2YeCmKOIdCaIPipEYi"), overwrite = T) # DOM_Synthesis_Data_Trim.csv
  # drive_download(as_id("https://drive.google.com/file/d/135aP9tmAcXS33nE9atYztIXZ1U3DknvE"), overwrite = T) # DOM_Synthesis_Mol_Trim.csv
  # drive_download(as_id("https://drive.google.com/file/d/1Gb-Z2IwHD127XVDHuXiYZZgmky2nBR3C"), overwrite = T) # DOM_Synthesis_Mol_Trim_Lambda.csv
  
  
  # aggregated data not used as aggregating is implemented below
  # drive_download(as_id("https://drive.google.com/file/d/1KSM47o6vfB4c1sjG0kLkoVMc3GhLZxHT"), overwrite = T) # Summary_Andrew_crosstab_wihout_iso_na.csv
  # drive_download(as_id("https://drive.google.com/file/d/1fVbjuFVfQTnYgkaBstWlwWzs1MKT-4Rm"), overwrite = T) # Summary_Andrew_crosstab_wihout_iso_wa.csv
  
  # metadata file as modified by Amy and Andrew
  # Download from Andrews Google Drive: https://drive.google.com/drive/folders/12qidcFRypaX5dmwUTTGVqsc0XwMo0wqM?usp=drive_link
  drive_download(as_id("https://drive.google.com/file/d/1h7GHrt_RQA-OKpIzHhAlbXfIEnVZ3Aor"), overwrite = T) # AH_metadata_merge.csv

}

```

# Read Original Data

```{r read original data, cache=T}

# data = read.csv("Merged_Processed_Data - 10-19-23.csv", check.names = F, row.names = 1) # use this if you have time to wait for the loading

# data = data.frame(fread("Merged_Processed_Data - 10-19-23.csv", check.names = F), row.names = 1) # original data
data = fread("Merged_Processed_Data - 10-19-23_ECF_ManyFilesFinal_Index.csv", check.names = F) # ECF modified data

# fast sorting by m/z 
setkeyv(data, "Calibrated m/z")


# mol = read.csv("Merged_Processed_Mol - 10-19-23.csv", check.names = F, row.names = 1) # use this if you have time to wait for the loading
mol = fread("Merged_Processed_Mol - 10-19-23.csv", check.names = F)

# fast sorting by m/z 
setkeyv(mol, "Calibrated m/z")


```
## Data Consistency Check
```{r data consistency check}

# check m/z value in both input tables
if (!base::setequal(data[, `Calibrated m/z`], mol[, `Calibrated m/z`])) {warning("Could not match all m/z values in data and mol table!"); cat("\n")} 

# collect mismatching m/z values
mz_to_remove_from_data <- fsetdiff(data[, .(`Calibrated m/z`)], mol[, .(`Calibrated m/z`)])
mz_to_remove_from_mol <- fsetdiff(mol[, .(`Calibrated m/z`)], data[, .(`Calibrated m/z`)])

message(paste0("Removing ", mz_to_remove_from_mol[, .N], " non-matching m/z values."), appendLF = T)

if (mz_to_remove_from_mol[, .N] > 0){

  removed_mz <- data.table(
    MF = mol[mz_to_remove_from_mol, `Molecular Formula`],
    mz_mol = mol[mz_to_remove_from_mol, `Calibrated m/z`],
    mz_data = data[mz_to_remove_from_data, `Calibrated m/z`]
  ) 
  cat(paste0("The following molecuar formulas will be discared \n", paste(unlist(removed_mz[, MF]), collapse = "\n"), "\n"))

  # remove m/z values not present in both data and mol objects
  data <- data[!`Calibrated m/z` %in% unlist(mz_to_remove_from_data)]
  mol <- mol[!`Calibrated m/z` %in% unlist(mz_to_remove_from_mol)]
  

  message(paste0(mol[, .N] , " molecular formulas remaining."), appendLF = T)
}

# check again
if (!base::setequal(data[, `Calibrated m/z`], mol[, `Calibrated m/z`])) {warning("Could still not match all m/z values in data and mol table!"); cat("\n")} 
    
rm(mz_to_remove_from_data, mz_to_remove_from_mol)

```

Each time the data is read, a different set of m/z values will be removed due to rounding issues during fread. 
TODO: Need to use different index column and not rownames

## Calculate Metrics
Using Bobs code:
```{r prepare original data for metric distribution, cache=T}

data <- data.frame(data, row.names = 1)
mol <- data.frame(mol, row.names = 1)

# Breaking apart by dataset
factors = data.frame(Sample = colnames(data),
                     Dataset = str_extract(colnames(data), "[^_]+"))

# Create empty object
dataset.data = data.frame(matrix(nrow = nrow(data), ncol = length(unique(factors$Dataset)),
                                 dimnames = list(row.names(data),
                                                 unique(factors$Dataset))),
                          check.names = F)

# Summarize presence/absence by dataset
for(i in 1:ncol(dataset.data)){
  # current dataset
  curr.data = colnames(dataset.data)[i]
  
  # Select matching samples
  temp.data = data[,which(factors$Dataset %in% curr.data)]
  
  # add in sums to pregenerated object
  dataset.data[,i] = rowSums(temp.data)
  
}

# Convert to presence/absence
dataset.data[dataset.data > 0] = 1

# Generate mol file with dataset information
dataset.mol = NULL

for(i in 1:ncol(dataset.data)){
  # find formulas
  w = which(dataset.data[,i] > 0)
  
  # find corresponding molecular information
  temp = mol[w,]
  
  # add dataset information
  temp$Dataset = colnames(dataset.data)[i]
  
  # add in to dataset.mol
  dataset.mol = rbind(dataset.mol, temp)
  
}

# cleanup
rm(factors, temp, temp.data, curr.data, w, i, dataset.data)

```

## Metric Distribution for Original Data
```{r plotting metric distribution of original data, message=FALSE, warning=FALSE}

# plotting metric distribution
dataset.mol %>% select(Dataset, AI_Mod, DBE_1, NOSC, OtoC_ratio, HtoC_ratio) %>%
  gather(Variable, Value, -Dataset) %>%
  ggplot(aes(x = Value, y = Dataset))+
  geom_density_ridges(aes(fill = Dataset), alpha = 0.5)+
  facet_wrap(.~Variable, scales = "free")+
  xlab("Derived DOM Metric")+
  theme_bw()

rm(dataset.mol)

```

# Trim Data

Using James's script and parameters; set chunk option to: "eval = false, include = false", if a different trimmed file is used below.  
```{r prepare trimed data from original data}

# QC data matrix

formula.drop.matrix = data.frame('singleton' = rep(NA,nrow(mol)),'P'= rep(NA,nrow(mol)), 'HtoC' = rep(NA,nrow(mol)), 'OtoC' = rep(NA,nrow(mol)), 'DBE_1' = rep(NA,nrow(mol)), 'DBE_O' = rep(NA,nrow(mol)), 'mass' = rep(NA,nrow(mol)),row.names = rownames(mol))

# define singletons
formula.occurence = rowSums(x = data)
head(formula.occurence)
singletons = formula.occurence[which(formula.occurence <= 1)]
head(singletons)
formula.drop.matrix$singleton[which(rownames(formula.drop.matrix) %in% names(singletons))] = 1

# define P formulas
p.formulas = rownames(mol)[which(mol$P > 0)]
formula.drop.matrix$P[which(rownames(formula.drop.matrix) %in% p.formulas)] = 1

# define masses out of range
bad.mass.formulas = rownames(mol)[which(as.numeric(rownames(mol)) < 200 | as.numeric(rownames(mol)) > 800)]
formula.drop.matrix$mass[which(rownames(formula.drop.matrix) %in% bad.mass.formulas)] = 1

# define O:C 
OC.formulas = rownames(mol)[which(mol$OtoC_ratio > 1)]
formula.drop.matrix$OtoC[which(rownames(formula.drop.matrix) %in% OC.formulas)] = 1

# define H:C
HC.formulas = rownames(mol)[which(mol$HtoC_ratio > 2.5 | mol$HtoC_ratio < 0.3)]
formula.drop.matrix$HtoC[which(rownames(formula.drop.matrix) %in% HC.formulas)] = 1

# define DBE minus C
DBE.C.formulas = rownames(mol)[which(mol$DBE_1 <= I(mol$C*0.6 - 15))]
formula.drop.matrix$DBE_1[which(rownames(formula.drop.matrix) %in% DBE.C.formulas)] = 1

# define DBE minus O
DBE.O.formulas = rownames(mol)[which(mol$DBE_O < I(-10) | mol$DBE_O > 10)]
formula.drop.matrix$DBE_O[which(rownames(formula.drop.matrix) %in% DBE.O.formulas)] = 1

# number of drops per filter
colSums(x = formula.drop.matrix,na.rm = T)

# number of filters per formula
filters.per.formula = rowSums(x = formula.drop.matrix,na.rm = T)
range(filters.per.formula)
formula.to.drop = names(filters.per.formula)[which(filters.per.formula > 0)]
length(formula.to.drop)

# trim down mol
mol.trim = mol[-which(rownames(mol) %in% formula.to.drop),]
dim(mol.trim)
summary(mol.trim)

# make Van-K plot
plot(mol.trim$HtoC_ratio ~ mol.trim$OtoC_ratio,cex=0.3)

# trim down the data file
data.trim = data[which(rownames(data) %in% rownames(mol.trim)),]
dim(data.trim)

# look at formula richness distribution
sample.richness = colSums(x = data.trim)
hist(sample.richness)

# needs to be True
identical(x = row.names(data.trim), y = row.names(mol.trim))

# cleanup
rm(formula.drop.matrix)
rm(bad.mass.formulas, DBE.C.formulas, DBE.O.formulas, filters.per.formula, formula.occurence, formula.to.drop, HC.formulas, OC.formulas, p.formulas, singletons, sample.richness)

# keep or remove original data
if (!keep_orig_data) {rm(data, mol)}


# only write or export if not knitting
if (!isTRUE(getOption('knitr.in.progress'))) { 
  
  # save trimmed files as csv
  fwrite(x = data.trim, file = "DOM_Synthesis_Data_Trim.csv", row.names = T)
  fwrite(x = mol.trim, file = "DOM_Synthesis_Mol_Trim.csv", row.names = T)

  # eventually push trimmed files back to Google Drive
  if (upload_files == T) {
    
    drive_update(as_id("https://drive.google.com/file/d/1AaJYEK0IM5TiZL2YeCmKOIdCaIPipEYi"), here("DOM_Synthesis_Data_Trim.csv")) # updates to James Google Drive
    drive_update(as_id("https://drive.google.com/file/d/135aP9tmAcXS33nE9atYztIXZ1U3DknvE"), here("DOM_Synthesis_Mol_Trim.csv")) # updates to James Google Drive
  
  }

}

```
## Test Trimmed Data
```{r warning=FALSE}

tmp <- as.data.table(data.trim)[, mz := row.names(data.trim)]

# Select data columns 
cols_to_test <- colnames(data.trim)

#check if some samples have no intensity value after trimming:
zero_mf <- melt(tmp[, lapply(.SD, sum, na.rm = T), .SDcols = cols_to_test], verbose = F) %>% .[value == 0, variable] %>% length(.)

if (zero_mf > 0) {
  message(paste0("There are ", zero_mf, " samples without MF after trimming."), appendLF = TRUE)
  removed_samples <- droplevels(melt(tmp[, lapply(.SD, sum, na.rm = T), .SDcols = cols_to_test], variable.name = "Sample") %>% .[value == 0, .(Sample)])
  cat(paste0("Samples \n", paste(unlist(removed_samples), collapse = "\n"), "\nwill be discared because they have no MF"))
  set(data.trim, , as.character(unlist(removed_samples)), NULL)
  message(paste0(length(colnames(data.trim)) , " samples remaining"), appendLF = TRUE)
  }

rm(tmp, cols_to_test)

```


## Not run: Read Trimmed Data

```{r read previously trimed data, eval=FALSE, cache=TRUE, include=FALSE}

# data.trim = read.csv("Merged_Processed_Data - 10-19-23.csv", check.names = F, row.names = 1) # use this if you have time to wait for the loading
data.trim = data.frame(fread("DOM_Synthesis_Data_Trim.csv", check.names = F), row.names = 1)

# mol.trim = read.csv("Merged_Processed_Mol - 10-19-23.csv", check.names = F, row.names = 1) # use this if you have time to wait for the loading
mol.trim = data.frame(fread("DOM_Synthesis_Mol_Trim.csv", check.names = F), row.names = 1)


```


```{r prepare trimed data for metric distribution, cache=T}

# Breaking apart by dataset
factors.trim = data.frame(Sample = colnames(data.trim),
                     Dataset = str_extract(colnames(data.trim), "[^_]+"))

# Create empty object
dataset.data.trim = data.frame(matrix(nrow = nrow(data.trim), ncol = length(unique(factors.trim$Dataset)),
                                 dimnames = list(row.names(data.trim),
                                                 unique(factors.trim$Dataset))),
                          check.names = F)

# Summarize presence/absence by dataset
for(i in 1:ncol(dataset.data.trim)){
  # current dataset
  curr.data = colnames(dataset.data.trim)[i]
  
  # Select matching samples
  temp.data = data.trim[,which(factors.trim$Dataset %in% curr.data)]
  
  # add in sums to pregenerated object
  dataset.data.trim[,i] = rowSums(temp.data)
  
  # cleanup
  rm(temp.data, curr.data)
}

# Convert to presence/absence
dataset.data.trim[dataset.data.trim > 0] = 1

# Generate mol file with dataset information
dataset.mol.trim = NULL

for(i in 1:ncol(dataset.data.trim)){
  # find formulas
  w = which(dataset.data.trim[,i] > 0)
  
  # find corresponding molecular information
  temp = mol.trim[w,]
  
  # add dataset information
  temp$Dataset = colnames(dataset.data.trim)[i]
  
  # add in to dataset.mol
  dataset.mol.trim = rbind(dataset.mol.trim, temp)
  
}

# cleanup 
rm(factors.trim, temp, w, i, dataset.data.trim)

```

## Metric Distribution for Trimmed Data
```{r plotting metric distribution, message=FALSE, warning=FALSE}

# plotting metric distribution
dataset.mol.trim %>% select(Dataset, AI_Mod, DBE_1, NOSC, OtoC_ratio, HtoC_ratio) %>%
  gather(Variable, Value, -Dataset) %>%
  ggplot(aes(x = Value, y = Dataset))+
  geom_density_ridges(aes(fill = Dataset), alpha = 0.5)+
  facet_wrap(.~Variable, scales = "free")+
  xlab("Derived DOM Metric")+
  theme_bw()

rm(dataset.mol.trim)

```


## Calculate addtl Mol Parameters "Lambda"
```{r load functions}

source("Lambda_Functions.R")

```

```{r run lambda calculations}

# Compute the lambda for chemical compositions on trimmed Mol file

# user parameters ------------------------------------------------------

# uncomment, if previously loaded data shall be used
# infile <-  read.csv("DOM_Synthesis_Mol_Trim.csv") # using
# temp <- infile

if (!exists("infile")) {temp <- copy(mol.trim)} # copy object if not loaded from local file system
mol.rownames <- rownames(temp)


CHEMICAL_ELEMENTS = c("C","H","N","O","P","S")

# main run -------------------------------------------------------------

info <- get_compositions(temp)
out <- get_lambda(info$chemical_compositions)

# build data frame
df <- as.data.frame(out)

# build col names
names <- rep("", 62)

names[1:12] <- c("delGcox0","delGd0","delGcat0","delGan0","delGdis0","lambda0",
                 "delGcox","delGd","delGcat","delGan","delGdis","lambda")

stoich_colnames <- c("donor","h2o","hco3","nh4","hpo4","hs","h","e","acceptor","biom")
stoich_types <- c("stoichD","stoichA","stoichCat","stoichAn","stoichMet")

for (i in 1:length(stoich_types)) {
  names[((i-1)*10+13):(i*10+12)] <- array(sapply(stoich_types[i], paste, stoich_colnames, sep="_"))
}
colnames(df) <- names
df['MolForm'] <- mol.trim[, "Molecular.Formula"] #info$formulas

df = df[,c("MolForm","delGcox0","delGd0","delGcat0","delGan0","delGdis0","lambda0","delGcox","delGd","delGcat","delGan","delGdis","lambda")]

# merge lambda with mol file
# rename column MolForm to Molecular.Formula 
names(df)[which(names(df) == "MolForm")] <- "Molecular.Formula"

temp = merge(temp, df, by = 'Molecular.Formula', sort = F)
rownames(temp) = mol.rownames

mol.trim <- temp


# only write or export if not knitting

if (!isTRUE(getOption('knitr.in.progress'))) { 
  
  # save file
  fwrite(temp, file = "DOM_Synthesis_Mol_Trim_Lambda.csv", row.names=T)
  
  # eventually push trimmed files back to Google Drive
  if (upload_files == T) {
    
    drive_update(as_id("https://drive.google.com/file/d/1Gb-Z2IwHD127XVDHuXiYZZgmky2nBR3C"), here("DOM_Synthesis_Mol_Trim_Lambda.csv")) # updates to James Google Drive
  
  }
}

# clean up
rm(info, out, temp, df, CHEMICAL_ELEMENTS, mol.rownames, i)

```

## TODO: Calculate further molecular formula parameters

```{r Calculate further molecular formula parameters}

# Placeholder


```


# Merge Data and Mol Tables
```{r merge data and mol}

# working with the trimmed data here for now:

tmp <- as.data.table(data.trim)[, mz := row.names(data.trim)]

tmp2 <- as.data.table(mol.trim)[, mz := row.names(mol.trim)]

tmp3 <- merge(tmp2, tmp, by = "mz")

# Select data columns 
cols_to_replace <- colnames(data.trim)

# check merging
if (all(cols_to_replace %in% colnames(tmp3)[(length(colnames(tmp2))+1):length(colnames(tmp3))])) {message(paste0("Merging successful"), appendLF = TRUE)} else {warning(paste0("The following samples were not merged: ", colnames(tmp3)[which(!cols_to_replace %in% colnames(tmp3)[(length(colnames(tmp2))+1):length(colnames(tmp3))])]))}

# Replace zeros with "NA" in data columns
tmp3[, (cols_to_replace) := lapply(.SD, function(x) ifelse(x == 0, NA, x)), .SDcols = cols_to_replace]

# renormalize to 0:1000
tmp3[, (cols_to_replace) := lapply(.SD, function(x) scales::rescale(x, to = c(0, 1000))), .SDcols = cols_to_replace]

tmp4 <- melt(tmp3, id.vars = colnames(tmp3)[!colnames(tmp3) %in% cols_to_replace], value.name = "Intensity", variable.name = "Sample", measure.vars = cols_to_replace, na.rm = T)

tmp4[, Dataset := str_extract(Sample, "[^_]+")]

# merged data table
data.mol.trim_long <- copy(tmp4)

# save to wide form
data.mol.trim_wide <- copy(tmp3) 

# clean up
rm(tmp, tmp2, tmp3, tmp4, cols_to_replace, zero_mf)

```


----------------------------------------------------------------------------------------------------

# Global Aggregates and QC
## MF Distribution and Exclusion Threshold
The threshold for excluding samples based on low MF is set here based on the overall distribution of the data
```{r}

summary(data.mol.trim_long[, .N, by = c("Sample")])

ggplot(data.mol.trim_long[, .N, by = c("Dataset", "Sample")], aes(x = Dataset, y = N, fill = Dataset)) + geom_violin() + theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(data.mol.trim_long[, .N, by = c("Sample")], aes(y = N, x = "All Data")) + geom_violin(trim = T) + theme(axis.text.x = element_text(angle = 45, hjust = 1))

Molecular_number_threshold <- 500

test <- data.mol.trim_long[, .N, by = c("Sample")][N < Molecular_number_threshold, .N]

message(paste0("There are ", test, " samples with less than ", Molecular_number_threshold, " molecular formulas"), appendLF = TRUE)
if (test > 0) {
  lowMF_samples <- droplevels(data.mol.trim_long[, .N, by = c("Sample")][N < Molecular_number_threshold, .(Sample)])
  cat(paste0("Samples \n", paste(unlist(lowMF_samples), collapse = "\n"), "\nwill be ignored because they have low # of MF", "\n"))
  message(paste0(data.mol.trim_long[, .N, by = c("Sample")][N >= Molecular_number_threshold, .N] , " samples remaining"), appendLF = TRUE)
  }

rm(test)

```



## Heteroatom Class Distributions
```{r factors for sorting}

source(here("factor_levels_for_sorting.R"))

```

```{r plot distribution by major element, message=FALSE, warning=FALSE}

het_check <- data.mol.trim_long[!Sample %in% lowMF_samples, .N, by = c("Heteroatom.Class", "Sample", "Dataset")][, .(mean_count = mean(N)), by = c("Heteroatom.Class", "Dataset")]

test <- copy(het_check)

for (het in c("CHO", "N", "S", "N+S")) {

  if (het == "CHO") {
  
      test_data <- test[!grepl(pattern = "N", Heteroatom.Class) & !grepl(pattern = "S", Heteroatom.Class), ] %>% dcast(., Heteroatom.Class~Dataset, value.var = "mean_count" , fill = 0)
            test_data[, Heteroatom.Class := factor(Heteroatom.Class, levels = fctr_lvls_cho)]

  }
  
    if (het == "N") {
  
      test_data <- test[grepl(pattern = "N", Heteroatom.Class) & !grepl(pattern = "S", Heteroatom.Class), ] %>% dcast(., Heteroatom.Class~Dataset, value.var = "mean_count" , fill = 0)
            test_data[, Heteroatom.Class := factor(Heteroatom.Class, levels = fctr_lvls_n)]

    }
  
      if (het == "S") {
  
      test_data <- test[!grepl(pattern = "N", Heteroatom.Class) & grepl(pattern = "S", Heteroatom.Class), ] %>% dcast(., Heteroatom.Class~Dataset, value.var = "mean_count" , fill = 0)
            test_data[, Heteroatom.Class := factor(Heteroatom.Class, levels = fctr_lvls_s)]

      }
  
      if (het == "N+S") {
  
      test_data <- test[grepl(pattern = "N", Heteroatom.Class) & grepl(pattern = "S", Heteroatom.Class), ] %>% dcast(., Heteroatom.Class~Dataset, value.var = "mean_count" , fill = 0)
      test_data[, Heteroatom.Class := factor(Heteroatom.Class, levels = fctr_lvls_ns)]

}

setkeyv(test_data, cols = "Heteroatom.Class")

print(heatmap(as.matrix(test_data, rownames = "Heteroatom.Class"), Rowv = NA, Colv = NA, revC = T, ylab = "Heteroatom.Class", xlab = "Dataset", scale = "none", main = het, keep.dendro = F))

}


rm(test, test_data, het, het_check)

```

----------------------------------------------------------------------------------------------------

# Adding More Data and Indices
## TODO: Include Diversity Metrics from Andrew 
```{r Diversity Metrics from Andrew}

# Placeholder for the code from Andrew

```

----------------------------------------------------------------------------------------------------

# Generate Aggregated Values
## Not run: Read Aggregated Values
file generated using Yuanbis code

```{r read aggregated values, eval=FALSE, include=FALSE}

# load files and do some renaming + remove samples with 0 MF
# Weighted averages
data.mol.trim.agg_wa <- fread("Summary_Andrew_crosstab_wihout_iso_wa.csv")
names(data.mol.trim.agg_wa)[which(names(data.mol.trim.agg_wa ) == "variable")] <- "Sample"
data.mol.trim.agg_wa <- data.mol.trim.agg_wa[Molecular_number > 0] %>% .[, Dataset := str_extract(Sample, "[^_]+")]

# Number averages
data.mol.trim.agg_na <- fread("Summary_Andrew_crosstab_wihout_iso_na.csv")
names(data.mol.trim.agg_na)[which(names(data.mol.trim.agg_na) == "variable")] <- "Sample"
data.mol.trim.agg_na <- data.mol.trim.agg_na[Molecular_number > 0] %>% .[, Dataset := str_extract(Sample, "[^_]+")]
names(data.mol.trim.agg_na) <- sub("_wa", "_na", names(data.mol.trim.agg_na))


```

## Calculate Aggregated Data
```{r}

source("aggregated_value_calculations.R")


```


```{r Calculate Aggregated Data}

# select samples columns
smp <- as.character(data.mol.trim_long[, unique(Sample)])
# select variable columns 
cols_to_use <- names(mol.trim)[-which(names(mol.trim) %in% c("Molecular.Formula", "Is.Isotopologue", "Heteroatom.Class", "m.z.Error..ppm.", "MolForm", "NtoP_ratio", "bs1_class", "bs2_class", "bs3_class"))] # removed NtoP_ratio for now, since the trimmed data does not contain this element currently.

# calculate the number of formulas
counts <- data.mol.trim_long[, .(Molecular_number = .N), by = "Sample"]

# start with intensity weighted averages
agg <- setNames(data.table(matrix(nrow = data.mol.trim_long[, uniqueN(Sample)], ncol = length(cols_to_use)+1)), c("Sample", cols_to_use))

agg[, Sample := data.mol.trim_long[, unique(Sample)]]

for (column in cols_to_use) {
print(paste0("processing ", column))
  agg[, (column) := CWM_trait(data.mol.trim_wide[, get(column)], data.mol.trim_wide[, .SD, .SDcols = smp], relative = F)]

}

data.mol.trim.agg_wa <- copy(agg)
data.mol.trim.agg_wa <- data.mol.trim.agg_wa[, Dataset := str_extract(Sample, "[^_]+")][counts, on = "Sample"]


# now calculate number based averages
agg <- setNames(data.table(matrix(nrow = data.mol.trim_long[, uniqueN(Sample)], ncol = length(cols_to_use)+1)), c("Sample", cols_to_use))

agg[, Sample := data.mol.trim_long[, unique(Sample)]]

tmp <- as.data.table(lapply(data.mol.trim_wide[, .SD, .SDcols = smp], function(x) as.integer(!is.na(x))))
tmp <- cbind(data.mol.trim_wide[, .SD, .SDcols = cols_to_use], tmp)


for (column in cols_to_use) {
print(paste0("processing ", column))
  agg[, (column) := CWM_trait(tmp[, get(column)], tmp[, .SD, .SDcols = smp], relative = F)]

}

data.mol.trim.agg_na <- copy(agg)
data.mol.trim.agg_na <- data.mol.trim.agg_na[, Dataset := str_extract(Sample, "[^_]+")][counts, on = "Sample"]

# clean up
rm(smp, cols_to_use, agg, counts, tmp)


```
TODO: Make aggregate value calculation code more efficient

# Read Metadata
```{r read metadata}

metadata <- fread("AH_metadata_merge.csv", na.strings = c("NA", ""))

# filter data without coreMS filename
metadata <- metadata[!is.na(coreMSname)]

# renaming
metadata[, Sample := coreMSname]


```


```{r merge metadata with aggregated data}

data.mol.trim.agg_na[, Sample := sub(".csv", "", Sample)]
data.mol.trim.agg_wa[, Sample := sub(".csv", "", Sample)]

data.mol.trim.agg_na.meta <- merge(data.mol.trim.agg_na, metadata, by = "Sample")
data.mol.trim.agg_wa.meta <- merge(data.mol.trim.agg_wa, metadata, by = "Sample")


```

The number of matches is lower than in the original files!
TODO: Check which samples are removed during merging of aggregated data with metadata

```{r quick overview of data basis}

data.mol.trim.agg_wa.meta[, .N, by = "type"]
data.mol.trim.agg_wa.meta[, .N, by = "Dataset"]

```

# Write and Upload Merged and Aggregated Data 
```{r Export merged and aggregated data}

# only write or export if not knitting

if (!isTRUE(getOption('knitr.in.progress'))) { 
  
  fwrite(data.mol.trim.agg_wa.meta, "DOM_Synthesis_Data.Agg_WA_Meta.csv")
  fwrite(data.mol.trim.agg_na.meta, "DOM_Synthesis_Data.Agg_NA_Meta.csv")
  fwrite(data.mol.trim_wide, "DOM_Synthesis_Data.Wide.csv")
  fwrite(data.mol.trim_long, "DOM_Synthesis_Data.Long.csv")
  
  if (upload_files == T) {
    
    drive_update(as_id("https://drive.google.com/file/d/1lwJAWumRScDbvvJ9MKrUJ8FZVAz9bP6P"), here("DOM_Synthesis_Data.Agg_WA_Meta.csv")) # currently pushes to Andrews Google Drive
    drive_update(as_id("https://drive.google.com/file/d/1vCKDNeTP01okIX_xMmKL-R9kiCrA9iYu"), here("DOM_Synthesis_Data.Agg_NA_Meta.csv")) # currently pushes to Andrews Google Drive
    # drive_update(as_id(), here("DOM_Synthesis_Data.Wide.csv")) # nothing to update yet
    # drive_update(as_id(), here("DOM_Synthesis_Data.Long.csv")) # nothing to update yet
    
  }
}

```




# QC Plots
## Weighted and Number Averages per Dataset
```{r plot wa distributions per dataset, message=FALSE, warning=FALSE}

cols_to_use <- c("HtoC_ratio", "OtoC_ratio", "Calculated.m.z", "AI_Mod")

test <- melt(data.mol.trim.agg_wa[Molecular_number > Molecular_number_threshold, .SD, .SDcols = c("Dataset", "Sample", cols_to_use)], value.name = "wa_value", id.vars = c("Dataset", "Sample"))

ggplot(test, aes(x = Dataset, y = wa_value)) + geom_boxplot() + facet_wrap("variable", scales = "free") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

rm(test, cols_to_use)
 
```

```{r plot na distributions per dataset, message=FALSE, warning=FALSE}

cols_to_use <- c("HtoC_ratio", "OtoC_ratio", "Calculated.m.z", "AI_Mod")

test <- melt(data.mol.trim.agg_na[Molecular_number > Molecular_number_threshold, .SD, .SDcols = c("Dataset", "Sample", cols_to_use)], value.name = "wa_value", id.vars = c("Dataset", "Sample"))

ggplot(test, aes(x = Dataset, y = wa_value)) + geom_boxplot() + facet_wrap("variable", scales = "free") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

rm(test, cols_to_use)
 
```

```{r correlate na with wa}

cols_to_use <- c("HtoC_ratio", "OtoC_ratio", "Calculated.m.z", "AI_Mod")

for (n in cols_to_use) {
  
  test <- merge(data.mol.trim.agg_na[Molecular_number > Molecular_number_threshold, .SD, .SDcols = c("Sample", "Dataset", n)], data.mol.trim.agg_wa[Molecular_number > Molecular_number_threshold, .SD, .SDcols = c("Sample", n)], by = "Sample")
  names(test) <- sub(".x", "_na", names(test))
  names(test) <- sub(".y", "_wa", names(test))
  
  print(ggplot(test, aes(x = get(paste0(n, "_wa")), y = get(paste0(n, "_na")), color = Dataset)) + geom_point() + labs(title = paste0(n, " (# MF > ", Molecular_number_threshold, ")")) + xlab(paste0(n, "_wa")) + ylab(paste0(n, "_na")))
  
  
}

rm(test, cols_to_use)

```



## Main Molecular Descriptors
```{r overall sample distributions}

# check dependency on # Assignments
ggplot(data.mol.trim.agg_wa, aes(x = Molecular_number, y = Calculated.m.z, color = Dataset)) + geom_point() + labs(title = paste0("All Samples"))
ggplot(data.mol.trim.agg_wa[Molecular_number >= Molecular_number_threshold], aes(x = OtoC_ratio, y = HtoC_ratio, color = Dataset)) + geom_point() + labs(title = paste0("Samples with more than ", Molecular_number_threshold, " assignments"))
ggplot(data.mol.trim.agg_wa[Molecular_number < Molecular_number_threshold], aes(x = OtoC_ratio, y = HtoC_ratio, color = Dataset)) + geom_point() + labs(title = paste0("Samples with less than ", Molecular_number_threshold, " assignments"))

```


